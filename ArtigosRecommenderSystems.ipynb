{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArtigosRecommenderSystems.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMxLWtH0rLW1"
      },
      "source": [
        "Bibliotecas usadas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHYfLGF2sTPh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import sklearn.metrics as metrics\n",
        "from surprise import SVD, SVDpp, Dataset, accuracy, Reader, BaselineOnly, KNNBaseline\n",
        "from surprise.model_selection import GridSearchCV, cross_validate\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N3C7haNh0nP"
      },
      "source": [
        "\n",
        "Cria uma coluna com o numero total de ratings que o artigo teve\n",
        "''' O motivo é para evitar artigos classificados com 5 estrelas, mas apenas por 1 ou alguns usuários.\n",
        "Esses artigos não são relevantes se forem populares o suficiente para que mais usuários os classifiquem.'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEe_PVHghxGY"
      },
      "source": [
        "df_artigos = pd.read_csv('shared_articles.csv')\n",
        "df_usuarios = pd.read_csv('users_iter.csv')\n",
        "\n",
        "filtro_tipo_evento_view = df_usuarios[\"eventType\"]==\"VIEW\"\n",
        "filtro_tipo_evento_like = df_usuarios[\"eventType\"]==\"LIKE\"\n",
        "\n",
        "data = pd.merge(left=df_artigos, right=df_usuarios.loc[filtro_tipo_evento_view].drop_duplicates(['personId','contentId']), on='contentId')\n",
        "\n",
        "num_ratings = pd.DataFrame(data.groupby('contentId').count()['rating']).reset_index()\n",
        "data = pd.merge(left=data, right=num_ratings, on='contentId')\n",
        "data.rename(columns={'rating_x': 'rating', 'rating_y': 'numRatings'}, inplace=True)\n",
        "\n",
        "matrix = data.pivot_table(\n",
        "    index='personId',\n",
        "    columns='contentId',\n",
        "    values='rating'\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQlPiCIMiJSl"
      },
      "source": [
        "# Correlações de pares\n",
        "'''\n",
        "As correlações de pares revelam relações de interesse potenciais. \n",
        "A correlação de pares é calculada entre linhas ou colunas do DataFrame.\n",
        "Como os valores da matriz são os ratings dos artigos, então a recomendação é relacionada diretamente ao rating,numero de rating e usuário.\n",
        "pega itens similares ao que for passado usando a correlação de pearson (do metodo .corrwith que usa pearson como padrão da lib Pandas)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrLZxfbTiXZ9"
      },
      "source": [
        "def pegar_artigos_similares_correlacao_pares(movie_title, n_ratings_filter=100, n_recommendations=10):\n",
        "    #print(matrix[movie_title])\n",
        "    similar = matrix.corrwith(matrix[movie_title])\n",
        "    corr_similar = pd.DataFrame(similar, columns=['correlacao'])\n",
        "    corr_similar.dropna(inplace=True)\n",
        "    \n",
        "    orig = data.copy()\n",
        "    \n",
        "    corr_with_movie = pd.merge(\n",
        "        left=corr_similar, \n",
        "        right=orig, \n",
        "        on='contentId')[['contentId', 'title', 'correlacao', 'numRatings']].drop_duplicates().reset_index(drop=True)\n",
        "    \n",
        "    result = corr_with_movie[corr_with_movie['numRatings'] > n_ratings_filter].sort_values(by='correlacao', ascending=False)\n",
        "    \n",
        "    return result.head(n_recommendations)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GqglEHPiZTi"
      },
      "source": [
        "print(pegar_artigos_similares_correlacao_pares(6044362651232258738));"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPyd9kXdolZD"
      },
      "source": [
        "title  correlacao  numRatings\n",
        "1206  Cinco competências comportamentais para você s...    1.000000         103\n",
        "918        10 Modern Software Over-Engineering Mistakes    0.740744         118\n",
        "535   Custo do Erro - Cinco motivos para investir em...    0.568914         121\n",
        "192   Ganhe 6 meses de acesso ao Pluralsight, maior ...    0.261114         142\n",
        "90    Ray Kurzweil: The world isn't getting worse - ...    0.224478         143\n",
        "1364  Psicóloga de Harvard diz que as pessoas julgam...    0.177282         140\n",
        "201                    Livro: Retrospectivas Divertidas    0.125102         193\n",
        "1317  Um bilhão de arquivos mostram quem vence a dis...    0.110088         148\n",
        "413   Former Google career coach shares a visual tri...   -0.024125         267\n",
        "1204                              O chefe é gay. E daí?   -0.075072         103"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-jTxrgMovfu"
      },
      "source": [
        "Como pode ser visto no resultado, o primeiro artigo é o proprio passado como parametro e o resto a correlação vai diminuindo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGDVuwKfid3q"
      },
      "source": [
        "# Content-based - TF-IDF\n",
        "'''\n",
        "As duas funções seguintes fazem recomendação a partir do metodo content-based TF-IDF que basicamente mede a importancia das palavras mas ignora as palavras mais frequentes como\n",
        "'the/a' (palavras pequenas)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExqHZSiUmnrI"
      },
      "source": [
        "def pegar_artigos_similares_porTitulo_Tf_Idf():\n",
        "    \n",
        "    tf=TfidfVectorizer()\n",
        "    tf_mat=tf.fit_transform(data['title'])\n",
        "\n",
        "    linear_kernel_ = linear_kernel(tf_mat, tf_mat)\n",
        "\n",
        "    similar=linear_kernel_[2].tolist()\n",
        "\n",
        "    ind=[]\n",
        "    val=[]\n",
        "    for i in range(len(similar)):\n",
        "        ind.append(i)\n",
        "        val.append(similar[i])    \n",
        "    \n",
        "    dic={'index':ind,'value':val}\n",
        "\n",
        "    rec_df=pd.DataFrame(dic)\n",
        "\n",
        "    #print(rec_df)\n",
        "\n",
        "    rec=rec_df.merge(data,on='index')   \n",
        "\n",
        "    a=rec.sort_values(by='value',ascending=False).head(10)\n",
        "    \n",
        "    print(data['title'][2])\n",
        "    print(a[['title', 'value']])\n",
        "\n",
        "def pegar_artigos_similares_porConteudo_Tf_Idf():\n",
        "    \n",
        "    tf=TfidfVectorizer()\n",
        "    tf_mat=tf.fit_transform(data['text'].fillna(''))\n",
        "\n",
        "    cosine_sim = cosine_similarity(tf_mat)\n",
        "\n",
        "    similar=cosine_sim[2].tolist()\n",
        "\n",
        "    ind=[]\n",
        "    val=[]\n",
        "    for i in range(len(similar)):\n",
        "        ind.append(i)\n",
        "        val.append(similar[i])    \n",
        "    \n",
        "    dic={'index':ind,'value':val}\n",
        "\n",
        "    rec_df=pd.DataFrame(dic)\n",
        "\n",
        "    #print(rec_df)\n",
        "\n",
        "    rec=rec_df.merge(data,on='index')   \n",
        "\n",
        "    a=rec.sort_values(by='value',ascending=False).head(10)\n",
        "\n",
        "    print(data['title'][2])\n",
        "    print(a[['title', 'value']])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYnoY6lum3CX"
      },
      "source": [
        "#colocar a coluna index e exclui duplicadas\n",
        "data = data.drop_duplicates(subset='title', keep=\"first\").reset_index()\n",
        "\n",
        "pegar_artigos_similares_porConteudo_Tf_Idf()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLaMD5BtpWfW"
      },
      "source": [
        "Google Data Center 360° Tour\n",
        "                                                 title     value\n",
        "85   Google Is Finally Redesigning Its Biggest Cash...  0.481396\n",
        "118       Practical End-to-End Testing with Protractor  0.392924\n",
        "268                Building a digital-banking business  0.390531\n",
        "296  App-only bank Atom just launched - here's what...  0.382130\n",
        "132  Google Cloud Datastore simplifies pricing, cut...  0.372017\n",
        "119  How Mature is Your Organization when it Comes ...  0.364058\n",
        "17   Using Gamified Hacking Challenges To Attract N...  0.354738\n",
        "201  Open Source Giant Red Hat Launches First Block...  0.352467\n",
        "348   Vidente reflete sobre os desafios contemporâneos  0.351830\n",
        "375       Humans Can Still Do One Thing Better Than AI  0.348447"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygIINw20pXdd"
      },
      "source": [
        "Resultado mais satisfatorio com relação a conteudo dos artigos. É passado como parametro o titulo \"Google Data Center 360° Tour\" e nos resultados a maioria dos artigos tem relação com o artigo informado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR5OzLaWnOOq"
      },
      "source": [
        "# Usando um classificador Bayesianos\n",
        "'''\n",
        "    Aqui basicamente vai pegar os artigos que o usuário deu 5 estrelas\n",
        "    usando a estrela como label e o text (conteudo) como feature\n",
        "    vamos treinar um modelo para tentar prever se o usuário vai gostar ou não de um certo artigo\n",
        "    seria como uma lista de 'algo que vc talvez goste'.\n",
        "    Como a base que temos é pequena (3000 artigos), a precisão da previsão é baixa (chegou no maximo em 20%)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cf5xL3GnP4z"
      },
      "source": [
        "def pegar_artigos_similares_porConteudo_Bayesiano():\n",
        "\n",
        "    count = CountVectorizer()\n",
        "\n",
        "    # separando feature de target (no nosso caso titulo ou conteudo do artigo e target seria a classificação dele (rating))\n",
        "    X = data['text']\n",
        "    y = data['rating']\n",
        "\n",
        "    # cria um conjunto de palavras a partir da coluna title ou text dos artigos\n",
        "    palavras_transformer = count.fit(X)\n",
        "    print(len(palavras_transformer.vocabulary_))\n",
        "  \n",
        "    # faz o transforme nos titulos\n",
        "    X = palavras_transformer.transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "    sm = SMOTE(random_state=2)\n",
        "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
        "\n",
        "    # Agora treinar o modelo usando Naive Bais Algorithm\n",
        "\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    modelo_Finalizado = open(\"Finalized_Model_NB.pkl\", \"wb\")\n",
        "    pickle.dump(nb, modelo_Finalizado)\n",
        "\n",
        "    # fazer predicoes no conjunto de teste\n",
        "    preds = nb.predict(X_test)\n",
        "        \n",
        "    # Validar o modelo usando confusion matrix e classification report\n",
        "    print(confusion_matrix(y_test, preds))\n",
        "    print('\\n')\n",
        "    print(classification_report(y_test, preds))\n",
        "    print(metrics.accuracy_score(y_test, preds))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C969yrmXnYXs"
      },
      "source": [
        "pegar_artigos_similares_porConteudo_Bayesiano()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNsvVjsMqbBM"
      },
      "source": [
        "[[273 379 305 280 429 384] \n",
        " [257 354 290 304 407 377] \n",
        " [271 364 317 261 466 378] \n",
        " [281 344 312 279 427 400] \n",
        " [308 344 316 262 375 369] \n",
        " [307 350 301 274 422 365]]\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.16      0.13      0.15      2050\n",
        "           1       0.17      0.18      0.17      1989\n",
        "           2       0.17      0.15      0.16      2057\n",
        "           3       0.17      0.14      0.15      2043\n",
        "           4       0.15      0.19      0.17      1974\n",
        "           5       0.16      0.18      0.17      2019\n",
        "\n",
        "    accuracy                           0.16     12132\n",
        "   macro avg       0.16      0.16      0.16     12132\n",
        "weighted avg       0.16      0.16      0.16     12132\n",
        "\n",
        "0.1618034948895483"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPiuN92GqsV_"
      },
      "source": [
        "Com a base de 3 mil artigos o resultado foi muito baixo, com apenas 16% de acuracia. Isso por causa que tem muito pouco artigo para fazer o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXtrs3mPnq4a"
      },
      "source": [
        "df_livros = pd.read_csv('GoodReads_100k_books.csv')\n",
        "\n",
        "df_livros['ratingAprox']  = df_livros.apply(lambda row: int(round(row.rating)) , axis = 1)\n",
        "\n",
        "#print(df_livros[['title', 'rating', 'ratingAprox', 'totalratings']].sort_values('ratingAprox'))\n",
        "\n",
        "def pegar_livros_similares_porConteudo_Bayesiano():\n",
        "    \n",
        "    count = CountVectorizer()\n",
        "\n",
        "    # separando feature de target (no nosso caso titulo ou conteudo do artigo e target seria a classificação dele (rating))\n",
        "    X = df_livros['desc'].apply(lambda x: np.str_(x))\n",
        "    print(X)\n",
        "    y = df_livros['ratingAprox']\n",
        "\n",
        "    # cria um conjunto de palavras a partir da coluna title ou text dos artigos\n",
        "    palavras_transformer = count.fit(X)\n",
        "    print(len(palavras_transformer.vocabulary_))\n",
        "  \n",
        "    # faz o transforme nos titulos\n",
        "    X = palavras_transformer.transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "    sm = SMOTE(random_state=2)\n",
        "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
        "\n",
        "    # Agora treinar o modelo usando Naive Bais Algorithm\n",
        "\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    modelo_Finalizado = open(\"Finalized_Model_NB_2.pkl\", \"wb\")\n",
        "    pickle.dump(nb, modelo_Finalizado)\n",
        "\n",
        "    # fazer predicoes no conjunto de teste\n",
        "    preds = nb.predict(X_test)\n",
        "        \n",
        "    # Validar o modelo usando confusion matrix e classification report\n",
        "    print(confusion_matrix(y_test, preds))\n",
        "    print('\\n')\n",
        "    print(classification_report(y_test, preds))\n",
        "    print(metrics.accuracy_score(y_test, preds))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvZ57ZEDn32T"
      },
      "source": [
        "pegar_livros_similares_porConteudo_Bayesiano()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4gryGwrp2si"
      },
      "source": [
        "[[    2     0     0    15   449     8]\n",
        " [    0     0     0     1    12     0]\n",
        " [    1     0     0     8    92     1]\n",
        " [    6     0    39   329  3202    67]\n",
        " [   11     2    89  1059 22984   487]\n",
        " [    2     1     3    44  1016    70]]\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.09      0.00      0.01       474\n",
        "           1       0.00      0.00      0.00        13\n",
        "           2       0.00      0.00      0.00       102\n",
        "           3       0.23      0.09      0.13      3643\n",
        "           4       0.83      0.93      0.88     24632\n",
        "           5       0.11      0.06      0.08      1136\n",
        "\n",
        "    accuracy                           0.78     30000\n",
        "   macro avg       0.21      0.18      0.18     30000\n",
        "weighted avg       0.71      0.78      0.74     30000\n",
        "\n",
        "0.7795"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRq_5sIHqQWf"
      },
      "source": [
        "Como pode ser visto, teve precisão de 77% para a base de dados dos livros com 100 mil livros. Mostrando que quanto maior a base melhor o resultado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFDa2KEqn77e"
      },
      "source": [
        "# METODOS USANDO COLLABORATIVE FILTERING\n",
        "'''\n",
        " A Filtragem Colaborativa é baseada na ideia de que usuários semelhantes a mim podem ser usados ​​para prever o quanto eu gostarei de um determinado\n",
        "  artigo que esses usuários visualizaram/gostaram mas eu não.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nqetBSPn9BT"
      },
      "source": [
        "def collaborativeFiltering_SDV(data_artigos_usuarios):\n",
        "    # Ver numeros totais de usuarios e artigos\n",
        "    n_users = data_artigos_usuarios['personId'].nunique()\n",
        "    n_artigos = data_artigos_usuarios['contentId'].nunique()\n",
        "\n",
        "    print('Numero de usuarios: ', n_users)\n",
        "    print('Numero de Artigos: ',n_artigos)\n",
        "\n",
        "    # Cria nova coluna id para o usuario\n",
        "    unique_user_id = pd.DataFrame(data_artigos_usuarios['personId'].unique(),columns =['personId']).reset_index()\n",
        "    unique_user_id['novo_id_usuario'] = unique_user_id['index']\n",
        "    del unique_user_id['index']\n",
        "\n",
        "    # cria nova coluna id para os artigos\n",
        "    unique_business_id = pd.DataFrame(data_artigos_usuarios['contentId'].unique(),columns =['contentId']).reset_index()\n",
        "    unique_business_id['novo_id_conteudo'] = unique_business_id['index']\n",
        "    del unique_business_id['index']\n",
        "\n",
        "    new_complete_df = data_artigos_usuarios.merge(unique_user_id,on='personId',how ='left')\n",
        "    new_complete_df = new_complete_df.merge(unique_business_id,on='contentId',how ='left')\n",
        "\n",
        "    df = new_complete_df[['novo_id_usuario','novo_id_conteudo','rating']]\n",
        "    print(df.head(10))\n",
        "    # Cria um objeto reader da lib surprise com escalas de rating de 0 a 5\n",
        "    reader = Reader(rating_scale=(0, 5))\n",
        "    # Carrega o dataset dos artigos em 5 folds\n",
        "    data = Dataset.load_from_df(df,reader)\n",
        "    #data.split(n_folds=5)\n",
        "\n",
        "    # Modelando usando SVD da lib surprise\n",
        "    algo = SVD()\n",
        "    # Valida usando como base RMSE e MAE\n",
        "    perf = cross_validate(algo, data, measures=['RMSE', 'MAE'])\n",
        "    #print(perf)\n",
        "\n",
        "    # Treina o dataset e acha os predict\n",
        "    trainset = data.build_full_trainset()\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    # predict o ratingo do usuario para o artigo id 0\n",
        "    svd_pred = algo.predict(str('1'),str('3'),r_ui=3,verbose=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5BuR0gjoXXH"
      },
      "source": [
        "collaborativeFiltering_SDV(data)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAMouDFDsD3o"
      },
      "source": [
        "{'test_rmse': array([1.78978369, 1.7766941 , 1.77971374, 1.78538199, 1.76183896]), 'fit_time': (1.5080702304840088, 1.4912209510803223, 1.5066721439361572, 1.4903011322021484, 1.5055773258209229), 'test_time': (0.06301712989807129, 0.041886091232299805, 0.041910648345947266, 0.061964988708496094, 0.0445401668548584)}\n",
        "user: 9          item: 3          r_ui = 3.00   est = 2.50   {'was_impossible': False}"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}
